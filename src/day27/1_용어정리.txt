[인공지능: AI]
    - 사람의 지능을 모방하는 넓은 범위의 기술 명칭, 머신러닝과 딥러닝을 포함
    - 다양한 문제를 해결하는 알고리즘, 규칙, 검색 방식, 추론 등을 학습할 수 있도록 만든 환경
[머신러닝: ML]
    - AI의 하위 분야, 데이터를 통해 패턴을 학습하여 예측
    - Scikit-learn 라이브러리
[딥러닝]
    - ML의 하위 분야, 신경망을 이용해 복잡한 데이터를 학습
    - TensorFlow(Keras), PyTorch 라이브러리

인공지능 > 머신러닝 > 딥러닝

[텐서플로]
    1. 구글이 개발한 오픈소스 머신러닝 및 딥러닝 프레임워크
    2. 딥러닝 연구 및 실무 환경에서 주로 사용

[하이퍼파라미터]
    - 파라미터/매개변수: 함수에 들어가는 인자값을 대입하는 변수
    - 개발자가 딥러닝 모델 학습 과정에서 직접 설정해야 하는 값/설정값
    - 튜닝: 최적값을 찾는 하이퍼파라미터 조정 과정 -> 다양한 조합의 하이퍼파라미터로 성능 평가 및 최적 조합을 찾기
        - 목적: 값 설정에 따라 모델 성능 및 결과가 달라지므로 중요하다
    - 종류: 1.학습률, 2.에포크, 3.배치 크기, 4.모델 구조, 5.정규화 방법, 6.옵티마이저, etc.

[과소적합 vs. 과대적합]
    - 과소적합
        - 모델이 너무 단순하다: 데이터 부족/너무 적은 수의 매개변수/낮은 차원의 모델
    - 과대적합
        - 모델이 너무 복잡하다: 훈련용 데이터 부족/너무 많은 수의 매개변수/너무 큰 차원 모델

    - 가장 이상적으로 학습된 지점을 찾는 것이 목표

[에포크]
    - 모델이 훈련 데이터셋을 완전히 학습한 횟수
    - 에포크 크기가 모델 성능에 큰 영향을 주므로 적절한 수가 필요하다
        - 너무 적을 시 과소적합, 너무 클 시 과대적합이 일어날 수 있다.

[손실함수]
    - 모델의 예측과 실제 값 간의 차이를 측정(오차)하여 성능을 평가하고 최적화 과정에서 가중치 조정 기준을 제공
    - 다양한 종류의 손실 함수가 있고 문제 유형에 따라 적절한 함수를 선택해야 한다
        e.g. 주로 사용) 회귀:MSE, MAE, 이진/다중 분류: 엔트로피

[경사하강법]
    - 손실 함수를 최소화하여 모델의 가중치를 최적화하는 알고리즘
    - 목표: 손실 함수의 값을 최소화하는 가중치를 찾기, 수학 개념인 미분을 사용
    - 그래디언트(기울기): 손실 함수의 기울기 = 현재 위치에서 함수의 증가율
    - 학습률: 기울기에 곱해 가중치를 수정한 크기를 결정
        - 학습률이 너무 크면 최적점에 도달하지 못하고 너무 작으면 특정 최적점에 빠질 수 있다 -> 적절한 학습률 필요

[옵티마이저]
    - 경사하강법과 같은 방법을 사용하여 모델의 성능을 개선한다
    - 실제값과 예측값 사이(손실 함수의 값)를 최소화하기 위해 매개변수를 조정하는 알고리즘
    - 종류:
        1. SGD: 매 반복마다 무작위 샘플을 이용하여 가중치를 업데이트하는 구조
        2. * Adam: 가장 많이 사용되는 옵티마이저, 기울기의 1차 평균과 2차 분산을 이용하여 학습 속도를 자동으로 조절

[딥러닝 프로세스]
    1. 데이터 로드 -> 2. 데이터 전처리 & 3. 데이터 분할 -> 4. 모델 생성 및 컴파일 -> 5. 훈련 -> 6. 검증 -> 7. 예측
                                                                       ^________튜닝_________/
[모델]
    1. Sequential API: 순차적 구조 모델
    2. Functional API: 복잡한 구조 모델

[노드(뉴런)]
    - 인공신경망에서 정보를 처리하고 전달하는 가장 기본적인 단위
    - 사람 뇌의 신경 세포(뉴런)을 모델로 해서 만들어졌다
    - 역할: 입력값, 계산 결과, 층 간의 전달

[Dense 레이어]
    - 인공신경망에서 가장 기본적인 레이어(층)
    - Dense 레이어에는 각 노드(뉴런)가 이전 레이어의 모든 노드와 연결된 상태: 완전 연결층
        입력층 -> 은닉층1 -> 은닉층2 -> 은닉층3 ... -> 출력층
        1. 입력층: 1차원 벡터 데이터만(Flatten() 결과 등) 입력받을 수 있는 층
        2. 은닉층: 입력층과 출력층 사이에 위치한 층/레이어
            - 입력 데이터에서 복잡한 패턴을 학습하는 공간
            - 데이터의 특징을 추출하고 더 정교한 정보를 만들기 위해 사용
        3. 출력층: 최종적으로 활성화 함수의 결과가 전달되는 층
            - 이진 분류: 활성화 함수 sigmoid
            - 다중 분류: 활성화 함수 sofmax

[활성화 함수]
    - 비선형적으로 변환해주는 함수
    - 종류: Relu, Signoid, Tanh, Leaky Relu
    - 사용처: 이미지 분류, 음성 인식, 자연어 처리 등 복잡한 문제 해결
    - 선형 vs 비선형적
        선형적 관계: 비례 관계일때 e.g. y = ax + b 직선 그래프
        비선형적 관계: 비례 관계가 아닐 때 e.g. 곡선 그래프

[원핫 벡터/인코딩]
    - 결과값이 1, 나머지는 0으로 표현하는 방식
    - e.g. 값이 1, 2, 3일 때 출력이 [1, 0, 0], [0, 1, 0], [0, 0, 1]
    - mnist 예시: 5를 출력할 때 [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]인 경우

[평가지표]
    1. accuracy: 분류 모델의 성능을 평가하는 자료  # 1에 가까울수로 좋은 기능